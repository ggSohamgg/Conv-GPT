{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://huggingface.co/spaces/nnsohamnn/Conv_GPT/resolve/main/Conv_GPT.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFzHlYx7ak1w",
        "outputId": "b4bdf820-018c-4c83-8d8f-386876f06aaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-30 19:58:06--  https://huggingface.co/spaces/nnsohamnn/Conv_GPT/resolve/main/Conv_GPT.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.114, 3.163.189.37, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/84/f0/84f0e4e90ae2a049be03456e0d9bdf6a60a4303d4a1c3993fe02d7554d78df6a/d07006505c691bae29120861fbc9dfe9ad3b75d4964e38b8445020991d4d6b17?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Conv_GPT.pth%3B+filename%3D%22Conv_GPT.pth%22%3B&Expires=1743368286&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzM2ODI4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzg0L2YwLzg0ZjBlNGU5MGFlMmEwNDliZTAzNDU2ZTBkOWJkZjZhNjBhNDMwM2Q0YTFjMzk5M2ZlMDJkNzU1NGQ3OGRmNmEvZDA3MDA2NTA1YzY5MWJhZTI5MTIwODYxZmJjOWRmZTlhZDNiNzVkNDk2NGUzOGI4NDQ1MDIwOTkxZDRkNmIxNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aVeq6J2A1EgCNq--hJgAM8I26FXMk%7ELfT2Yf64MdjEq7HtNaf2taaxdVPefULZrfggE6V6YNf3RTGpDaox-ybQtpYliZdu7-oFOWTlGE2AIRz7DIlsNzOtI6MbRGuRu-H9osUwVCqAVQnSTgzTV35lleVUol6m2U9gTeeiEJUZwUkjHJZUMIhpKq7NrWCiv%7E1lcUcZ-eR5raM7LaeMPStPOiH%7ETw6zLJ2t9s5F4DOUGCXKiEuDYN2AFZkcnqivqwH2G1OJLAymODQJ9ndCMELJqJWa3gUtgzOMSbR9sD0InJ4UWgQ5DiADGMPIELVRrGQeirn2IMffoojyQLJpgOYg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-30 19:58:06--  https://cdn-lfs-us-1.hf.co/repos/84/f0/84f0e4e90ae2a049be03456e0d9bdf6a60a4303d4a1c3993fe02d7554d78df6a/d07006505c691bae29120861fbc9dfe9ad3b75d4964e38b8445020991d4d6b17?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Conv_GPT.pth%3B+filename%3D%22Conv_GPT.pth%22%3B&Expires=1743368286&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzM2ODI4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzg0L2YwLzg0ZjBlNGU5MGFlMmEwNDliZTAzNDU2ZTBkOWJkZjZhNjBhNDMwM2Q0YTFjMzk5M2ZlMDJkNzU1NGQ3OGRmNmEvZDA3MDA2NTA1YzY5MWJhZTI5MTIwODYxZmJjOWRmZTlhZDNiNzVkNDk2NGUzOGI4NDQ1MDIwOTkxZDRkNmIxNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aVeq6J2A1EgCNq--hJgAM8I26FXMk%7ELfT2Yf64MdjEq7HtNaf2taaxdVPefULZrfggE6V6YNf3RTGpDaox-ybQtpYliZdu7-oFOWTlGE2AIRz7DIlsNzOtI6MbRGuRu-H9osUwVCqAVQnSTgzTV35lleVUol6m2U9gTeeiEJUZwUkjHJZUMIhpKq7NrWCiv%7E1lcUcZ-eR5raM7LaeMPStPOiH%7ETw6zLJ2t9s5F4DOUGCXKiEuDYN2AFZkcnqivqwH2G1OJLAymODQJ9ndCMELJqJWa3gUtgzOMSbR9sD0InJ4UWgQ5DiADGMPIELVRrGQeirn2IMffoojyQLJpgOYg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.172.170.40, 18.172.170.61, 18.172.170.105, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.172.170.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358490096 (342M) [binary/octet-stream]\n",
            "Saving to: ‘Conv_GPT.pth’\n",
            "\n",
            "Conv_GPT.pth        100%[===================>] 341.88M   229MB/s    in 1.5s    \n",
            "\n",
            "2025-03-30 19:58:08 (229 MB/s) - ‘Conv_GPT.pth’ saved [358490096/358490096]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew0XlyBUav2Y",
        "outputId": "a389568c-7766-4047-ab70-b8b8d37264c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "8ufbfjipar7j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "vocab_size = tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdr98vtzazPc",
        "outputId": "91267371-9c7f-4f3d-f9e9-c87e5f4b0198"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x, mask=None, padding_mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 1, -1e4)\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(padding_mask, -1e4)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
        "        out = self.out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(hidden_size, num_heads, dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 4 * hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * hidden_size, hidden_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, padding_mask=None):\n",
        "        x = self.ln1(x)\n",
        "        attn_out = self.attn(x, mask, padding_mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        x = self.ln2(x)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=512, num_layers=6, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = nn.Embedding(512, hidden_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_ln = nn.LayerNorm(hidden_size)\n",
        "        self.head = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, padding_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
        "        x = self.token_embedding(input_ids) + self.pos_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal_mask, padding_mask)\n",
        "\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "gsIe9FJdaSgP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "num_layers = 12\n",
        "num_heads = 16\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerModel(vocab_size=vocab_size, hidden_size=hidden_size,\n",
        "                         num_layers=num_layers, num_heads=num_heads,\n",
        "                         dropout=dropout).to(device)"
      ],
      "metadata": {
        "id": "iwXJaSXkaSnP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Conv_GPT.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "print(\"Pre-trained model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dxo05nfaSqX",
        "outputId": "c889b066-2562-4368-96f0-422c8c9b5a47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"blended_skill_talk\", split=\"train\")\n",
        "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "\n",
        "\n",
        "max_examples = 120\n",
        "dataset = dataset.select(range(min(len(dataset), max_examples)))\n",
        "print(f\"Using {len(dataset)} examples for fine-tuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcTyyOaxaSs8",
        "outputId": "9fba5639-6654-4dab-862c-c50eef4b470a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 4819 examples\n",
            "Using 120 examples for fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dialogue(example):\n",
        "    formatted = []\n",
        "    for i, utterance in enumerate(example['guided_messages']):\n",
        "        if not utterance.strip():\n",
        "            continue\n",
        "        speaker = \"User: \" if i % 2 == 0 else \"Assistant: \"\n",
        "        formatted.append(speaker + utterance)\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "dialogues = []\n",
        "for example in tqdm(dataset, desc=\"Formatting dialogues\"):\n",
        "    dialogue = format_dialogue(example)\n",
        "    dialogues.append(dialogue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sodkAN6saSvY",
        "outputId": "6f04973e-ba0a-4a03-92a1-e7dae591e7de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Formatting dialogues: 100%|██████████| 120/120 [00:00<00:00, 5640.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DialogueDataset(Dataset):\n",
        "    def __init__(self, dialogues):\n",
        "        self.sequences = []\n",
        "        for dialogue in tqdm(dialogues, desc=\"Tokenizing\"):\n",
        "            tokenized = tokenizer(dialogue, max_length=512, truncation=True, return_tensors='pt')['input_ids'][0]\n",
        "            self.sequences.append(tokenized)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    lengths = [len(seq) for seq in batch]\n",
        "    max_len = max(lengths)\n",
        "    padded_seqs = torch.full((len(batch), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, seq in enumerate(batch):\n",
        "        padded_seqs[i, :len(seq)] = seq\n",
        "    padding_mask = padded_seqs == tokenizer.pad_token_id\n",
        "    return padded_seqs, padding_mask"
      ],
      "metadata": {
        "id": "sXlEC592aSyf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_dataset = DialogueDataset(dialogues)\n",
        "fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, eps = 1e-7)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J7YxetAbR5w",
        "outputId": "3163c8bd-2098-45f5-92fe-6cf3d232755e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing: 100%|██████████| 120/120 [00:00<00:00, 801.65it/s]\n",
            "<ipython-input-11-30b5e2e272e1>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, scaler):\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for input_ids, padding_mask in tqdm(loader, desc=\"Fine-tuning\"):\n",
        "        input_ids, padding_mask = input_ids.to(device), padding_mask.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(input_ids, padding_mask)\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = input_ids[:, 1:].contiguous()\n",
        "            loss = loss_fn(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        preds = shift_logits.argmax(dim=-1)\n",
        "        mask = shift_labels != tokenizer.pad_token_id\n",
        "        correct = (preds == shift_labels) & mask\n",
        "        acc = correct.sum().float() / mask.sum().float()\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc.item()\n",
        "    return total_loss / len(loader), total_acc / len(loader)"
      ],
      "metadata": {
        "id": "Y6s-m4qJbUvQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, padding_mask in tqdm(loader, desc=\"Evaluating\"):\n",
        "            input_ids, padding_mask = input_ids.to(device), padding_mask.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(input_ids, padding_mask)\n",
        "                shift_logits = logits[:, :-1, :].contiguous()\n",
        "                shift_labels = input_ids[:, 1:].contiguous()\n",
        "                loss = loss_fn(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
        "            preds = shift_logits.argmax(dim=-1)\n",
        "            mask = shift_labels != tokenizer.pad_token_id\n",
        "            correct = (preds == shift_labels) & mask\n",
        "            acc = correct.sum().float() / mask.sum().float()\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc.item()\n",
        "    return total_loss / len(loader), total_acc / len(loader)"
      ],
      "metadata": {
        "id": "p78q9E2wbXHd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, prompt, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    generated_ids = input_ids\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(generated_ids, padding_mask=None)\n",
        "            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "            if next_token.item() == tokenizer.eos_token_id or tokenizer.decode(next_token.item()) == '\\n':\n",
        "                break\n",
        "    response = tokenizer.decode(generated_ids[0, len(input_ids[0]):])\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "KpdyCzEjbb5T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.85 * len(fine_tune_dataset))\n",
        "val_size = len(fine_tune_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(fine_tune_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "gGHNR0Bubf8-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fff1DklgbidF",
        "outputId": "f90771cb-9190-4e20-b3d5-ab813eca8c0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFine-tuning:   0%|          | 0/26 [00:00<?, ?it/s]<ipython-input-12-2aa961f182cc>:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Fine-tuning: 100%|██████████| 26/26 [00:02<00:00, 10.22it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 46.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=6.3302, Val Loss=5.5159, Train Acc=0.2218, Val Acc=0.2405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning: 100%|██████████| 26/26 [00:02<00:00, 12.53it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 55.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss=4.9039, Val Loss=5.1479, Train Acc=0.2702, Val Acc=0.2617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning: 100%|██████████| 26/26 [00:02<00:00, 11.97it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 47.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss=4.3426, Val Loss=4.9804, Train Acc=0.3015, Val Acc=0.2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning: 100%|██████████| 26/26 [00:02<00:00, 11.54it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 37.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss=3.9192, Val Loss=4.9306, Train Acc=0.3369, Val Acc=0.2775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning: 100%|██████████| 26/26 [00:02<00:00, 11.35it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 53.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss=3.5134, Val Loss=4.9883, Train Acc=0.3768, Val Acc=0.2706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_path = \"Conv_GPT_finetuned_blended_skill.pth\"\n",
        "torch.save(model.state_dict(), fine_tuned_model_path)\n",
        "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioLGgJLxbrjH",
        "outputId": "db79f75c-3d5a-4605-9c46-a1ac3cf3bf9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model saved to Conv_GPT_finetuned_blended_skill.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"User: Do you like books?\\nAssistant:\",\n",
        "    \"User: How much the ticket cost?\\nAssistant:\",\n",
        "    \"User: What is your favorite hobby?.\\nAssistant:\",\n",
        "    \"User: What are you doing later?\\nAssistant:\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the fine-tuned model:\")\n",
        "for prompt in test_prompts:\n",
        "    response = generate_text(model, prompt, max_new_tokens=100)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BnhLbUvdGLQ",
        "outputId": "c329ec29-f7ff-459d-f13e-7162e6c7da3f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing the fine-tuned model:\n",
            "\n",
            "Prompt: User: Do you like books?\n",
            "Assistant:\n",
            "Response: I like reading books. I like reading books.\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: User: How much the ticket cost?\n",
            "Assistant:\n",
            "Response: I have a ticket for a week.\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: User: What is your favorite hobby?.\n",
            "Assistant:\n",
            "Response: I like reading it as well but I enjoy watching reading and watching movies.\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: User: What are you doing later?\n",
            "Assistant:\n",
            "Response: I'm going to eat and eat a lot of fruit .\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}