{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cu3nVmqsdUTB",
        "outputId": "4d010e20-1d28-4a4c-b7c1-db47de34cfc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install datasets transformers\n",
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import torch.optim as optim\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "nWvramPrdrDs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_len):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, max_len, embed_size))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        seq_len = token_ids.size(1)\n",
        "        token_emb = self.token_embedding(token_ids)\n",
        "        pos_emb = self.position_embedding[:, :seq_len, :].to(token_emb.device)\n",
        "        return token_emb + pos_emb\n",
        "\n"
      ],
      "metadata": {
        "id": "2lx6jte3ep-U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, max_seq_len=256):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
        "        self.register_buffer(\"mask\", mask.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        assert T <= self.max_seq_len, f\"Sequence length {T} exceeds maximum {self.max_seq_len}\"\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn_mask = self.mask[:, :T, :T]\n",
        "        attn_scores = attn_scores.masked_fill(attn_mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        out = (attn_probs @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "CXUL4dhlJTjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim=3072, max_seq_len=256):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = CasualSelfAttention(embed_dim, num_heads, max_seq_len)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "0Z6_xtxCKB-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size=50257, embed_dim=768, num_heads=12, num_layers=12, seq_len=1024):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = Embedding(vocab_size, embed_dim, seq_len)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, max_seq_len=seq_len) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if torch.max(x) >= self.vocab_size:\n",
        "            raise ValueError(f\"Input contains token IDs >= vocab_size ({self.vocab_size})\")\n",
        "        x = self.embedding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "DFRJplXiKB7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"bavard/personachat_truecased\")\n",
        "train_dataset = dataset[\"train\"].select(range(10000))  # Take first 10,000 examples\n",
        "test_dataset = dataset[\"validation\"].select(range(1000))  # Take first 1,000 examples\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"Initializing tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Print tokenizer info\n",
        "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Tokenizer pad token ID: {tokenizer.pad_token_id}\")\n",
        "print(f\"Tokenizer eos token ID: {tokenizer.eos_token_id}\")\n",
        "\n",
        "# Sample dataset to check structure\n",
        "sample = train_dataset[0]\n",
        "print(\"Keys in the dataset example:\", sample.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AynZN6ljetU6",
        "outputId": "cae03bdb-5121-414e-d3e9-0c4a5265a002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing tokenizer...\n",
            "Tokenizer vocabulary size: 50257\n",
            "Tokenizer pad token ID: 50256\n",
            "Tokenizer eos token ID: 50256\n",
            "Keys in the dataset example: dict_keys(['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PersonaChatDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=256):\n",
        "        self.data = []\n",
        "        for example in dataset:\n",
        "            persona = \" \".join(example[\"personality\"])\n",
        "            dialogue = \" \".join(example[\"history\"])\n",
        "            text = f\"Persona: {persona} Dialogue: {dialogue}\"\n",
        "            tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "            self.data.append(tokens[\"input_ids\"].squeeze(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "ZYhZE_nCew2r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating datasets...\")\n",
        "train_data = PersonaChatDataset(train_dataset, tokenizer, max_length=256)\n",
        "test_data = PersonaChatDataset(test_dataset, tokenizer, max_length=256)\n",
        "\n",
        "sample_tokens = train_data[0]\n",
        "print(f\"Sample tokens - min: {sample_tokens.min().item()}, max: {sample_tokens.max().item()}\")\n",
        "print(f\"Sample tokens shape: {sample_tokens.shape}\")\n",
        "if sample_tokens.max() >= tokenizer.vocab_size:\n",
        "    print(f\"WARNING: Found token IDs >= vocab_size ({tokenizer.vocab_size})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf5Yd5abezeD",
        "outputId": "69253df2-3987-4413-a9c5-4b893685aa85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets...\n",
            "Sample tokens - min: 11, max: 50256\n",
            "Sample tokens shape: torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "print(\"Initializing model...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = GPT(vocab_size=vocab_size, embed_dim=384, num_heads=12, num_layers=12, seq_len=256).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n",
        "scaler = GradScaler()\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "dI9w7Tzee4M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracies = []\n",
        "best_test_loss = float('inf')\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    progress = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                   bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}',\n",
        "                   leave=True)\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        try:\n",
        "            # Ensure batch is within vocab range\n",
        "            if batch.max() >= vocab_size:\n",
        "                batch = torch.clamp(batch, max=vocab_size-1)\n",
        "\n",
        "            inputs = batch.to(device)\n",
        "            targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "                outputs = model(inputs[:, :-1])\n",
        "                # Use reshape instead of view for safety\n",
        "                loss = F.cross_entropy(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "            # Use gradient scaling for mixed precision training\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Clip gradients to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=-1)\n",
        "                correct_train += (preds == targets).sum().item()\n",
        "                total_train += targets.numel()\n",
        "\n",
        "            # Update progress bar with current loss and accuracy\n",
        "            current_loss = epoch_train_loss / (i + 1)\n",
        "            current_acc = correct_train / total_train if total_train > 0 else 0\n",
        "            progress.set_postfix(loss=f\"{current_loss:.4f}\", acc=f\"{current_acc:.4f}\")\n",
        "            progress.update(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i}: {e}\")\n",
        "            # Skip this batch and continue with the next one\n",
        "            continue\n",
        "\n",
        "    progress.close()\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    epoch_test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            try:\n",
        "                # Ensure batch is within vocab range\n",
        "                if batch.max() >= vocab_size:\n",
        "                    batch = torch.clamp(batch, max=vocab_size-1)\n",
        "\n",
        "                inputs = batch.to(device)\n",
        "                targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "                    outputs = model(inputs[:, :-1])\n",
        "                    # Use reshape instead of view for safety\n",
        "                    loss = F.cross_entropy(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "                    epoch_test_loss += loss.item()\n",
        "\n",
        "                    preds = outputs.argmax(dim=-1)\n",
        "                    correct += (preds == targets).sum().item()\n",
        "                    total += targets.numel()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in evaluation batch: {e}\")\n",
        "                # Skip this batch and continue with the next one\n",
        "                continue\n",
        "\n",
        "    avg_test_loss = epoch_test_loss / len(test_loader) if len(test_loader) > 0 else float('inf')\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    test_losses.append(avg_test_loss)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_test_loss < best_test_loss:\n",
        "        best_test_loss = avg_test_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved best model with Test Loss: {best_test_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TZ-mV059e8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Test Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T_8o8iymfoP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading best model for text generation...\")\n",
        "best_model = GPT(vocab_size=vocab_size, embed_dim=384, num_heads=8, num_layers=6, seq_len=256).to(device)\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "best_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zKVulD4fsNz",
        "outputId": "3bd967ef-3de9-46be-93a6-8b643e216958"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model for text generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-168005365357>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(\n",
              "    (token_embedding): Embedding(50257, 384)\n",
              "  )\n",
              "  (transformer_blocks): ModuleList(\n",
              "    (0-5): 6 x TransformerBlock(\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): CasualSelfAttention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=384, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=384, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=384, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get model predictions\n",
        "            outputs = model(input_ids)\n",
        "            next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the next token to input_ids\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "BgSf1aDLfw8_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Persona: Hello how was your day. Dialogue:\"\n",
        "generated_text = generate_text(best_model, tokenizer, prompt, max_length=20)\n",
        "print(f\"Generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv7CvMU2fy-q",
        "outputId": "ef8f43c1-2b3b-4f3c-c02c-fa1617daf938"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Persona: Hello how was your day. Dialogue: Drop. I hate Hurricanes? My two I'm a college mer at poll. I'm great years\n"
          ]
        }
      ]
    }
  ]
}